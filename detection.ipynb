{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EN9x84c4cgcy",
        "outputId": "78c379c6-7a76-48c8-96f3-9fa3902b1d77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py:75: UserWarning: The model does not have any trainable weights.\n",
            "  warnings.warn(\"The model does not have any trainable weights.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Batch: 1/82, d_loss_real=0.715, d_loss_fake=0.704, g_loss=0.704\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x7bc78453a3b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x7bc784505990> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Batch: 2/82, d_loss_real=0.707, d_loss_fake=0.703, g_loss=0.703\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 3/82, d_loss_real=0.706, d_loss_fake=0.704, g_loss=0.704\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 4/82, d_loss_real=0.705, d_loss_fake=0.703, g_loss=0.703\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 5/82, d_loss_real=0.705, d_loss_fake=0.703, g_loss=0.703\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 6/82, d_loss_real=0.704, d_loss_fake=0.704, g_loss=0.704\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 7/82, d_loss_real=0.704, d_loss_fake=0.704, g_loss=0.704\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Epoch: 1, Batch: 8/82, d_loss_real=0.704, d_loss_fake=0.704, g_loss=0.704\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Epoch: 1, Batch: 9/82, d_loss_real=0.704, d_loss_fake=0.704, g_loss=0.704\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 10/82, d_loss_real=0.704, d_loss_fake=0.704, g_loss=0.704\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 11/82, d_loss_real=0.704, d_loss_fake=0.704, g_loss=0.704\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 12/82, d_loss_real=0.705, d_loss_fake=0.705, g_loss=0.705\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 13/82, d_loss_real=0.705, d_loss_fake=0.705, g_loss=0.705\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 14/82, d_loss_real=0.705, d_loss_fake=0.706, g_loss=0.706\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 15/82, d_loss_real=0.706, d_loss_fake=0.707, g_loss=0.707\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Epoch: 1, Batch: 16/82, d_loss_real=0.707, d_loss_fake=0.708, g_loss=0.708\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Epoch: 1, Batch: 17/82, d_loss_real=0.708, d_loss_fake=0.709, g_loss=0.709\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Epoch: 1, Batch: 18/82, d_loss_real=0.709, d_loss_fake=0.710, g_loss=0.710\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Epoch: 1, Batch: 19/82, d_loss_real=0.710, d_loss_fake=0.712, g_loss=0.712\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Epoch: 1, Batch: 20/82, d_loss_real=0.712, d_loss_fake=0.713, g_loss=0.713\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Epoch: 1, Batch: 21/82, d_loss_real=0.713, d_loss_fake=0.715, g_loss=0.715\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 22/82, d_loss_real=0.715, d_loss_fake=0.716, g_loss=0.716\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 23/82, d_loss_real=0.716, d_loss_fake=0.718, g_loss=0.718\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 24/82, d_loss_real=0.718, d_loss_fake=0.720, g_loss=0.720\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 25/82, d_loss_real=0.719, d_loss_fake=0.721, g_loss=0.721\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 26/82, d_loss_real=0.721, d_loss_fake=0.723, g_loss=0.723\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 27/82, d_loss_real=0.723, d_loss_fake=0.725, g_loss=0.725\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
            "Epoch: 1, Batch: 28/82, d_loss_real=0.724, d_loss_fake=0.726, g_loss=0.726\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 29/82, d_loss_real=0.726, d_loss_fake=0.728, g_loss=0.728\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 30/82, d_loss_real=0.728, d_loss_fake=0.730, g_loss=0.730\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 31/82, d_loss_real=0.730, d_loss_fake=0.732, g_loss=0.732\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 32/82, d_loss_real=0.731, d_loss_fake=0.733, g_loss=0.733\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 33/82, d_loss_real=0.733, d_loss_fake=0.735, g_loss=0.735\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Epoch: 1, Batch: 34/82, d_loss_real=0.735, d_loss_fake=0.737, g_loss=0.737\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
            "Epoch: 1, Batch: 35/82, d_loss_real=0.736, d_loss_fake=0.738, g_loss=0.738\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 36/82, d_loss_real=0.738, d_loss_fake=0.740, g_loss=0.740\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 37/82, d_loss_real=0.739, d_loss_fake=0.741, g_loss=0.741\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 38/82, d_loss_real=0.741, d_loss_fake=0.743, g_loss=0.743\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 39/82, d_loss_real=0.742, d_loss_fake=0.744, g_loss=0.744\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 40/82, d_loss_real=0.744, d_loss_fake=0.746, g_loss=0.746\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 41/82, d_loss_real=0.745, d_loss_fake=0.747, g_loss=0.747\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 42/82, d_loss_real=0.747, d_loss_fake=0.749, g_loss=0.749\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 43/82, d_loss_real=0.748, d_loss_fake=0.750, g_loss=0.750\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 44/82, d_loss_real=0.749, d_loss_fake=0.751, g_loss=0.751\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
            "Epoch: 1, Batch: 45/82, d_loss_real=0.751, d_loss_fake=0.752, g_loss=0.752\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 46/82, d_loss_real=0.752, d_loss_fake=0.754, g_loss=0.754\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 47/82, d_loss_real=0.753, d_loss_fake=0.755, g_loss=0.755\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 48/82, d_loss_real=0.754, d_loss_fake=0.756, g_loss=0.756\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 49/82, d_loss_real=0.756, d_loss_fake=0.757, g_loss=0.757\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 50/82, d_loss_real=0.757, d_loss_fake=0.759, g_loss=0.759\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 51/82, d_loss_real=0.758, d_loss_fake=0.760, g_loss=0.760\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 52/82, d_loss_real=0.759, d_loss_fake=0.761, g_loss=0.761\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 53/82, d_loss_real=0.760, d_loss_fake=0.762, g_loss=0.762\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 54/82, d_loss_real=0.761, d_loss_fake=0.763, g_loss=0.763\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Epoch: 1, Batch: 55/82, d_loss_real=0.762, d_loss_fake=0.764, g_loss=0.764\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Epoch: 1, Batch: 56/82, d_loss_real=0.763, d_loss_fake=0.765, g_loss=0.765\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 57/82, d_loss_real=0.764, d_loss_fake=0.766, g_loss=0.766\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 58/82, d_loss_real=0.766, d_loss_fake=0.767, g_loss=0.767\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 59/82, d_loss_real=0.767, d_loss_fake=0.768, g_loss=0.768\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Epoch: 1, Batch: 60/82, d_loss_real=0.768, d_loss_fake=0.769, g_loss=0.769\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Epoch: 1, Batch: 61/82, d_loss_real=0.769, d_loss_fake=0.770, g_loss=0.770\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Epoch: 1, Batch: 62/82, d_loss_real=0.770, d_loss_fake=0.771, g_loss=0.771\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 63/82, d_loss_real=0.770, d_loss_fake=0.772, g_loss=0.772\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 64/82, d_loss_real=0.771, d_loss_fake=0.773, g_loss=0.773\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 65/82, d_loss_real=0.772, d_loss_fake=0.773, g_loss=0.773\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 66/82, d_loss_real=0.773, d_loss_fake=0.774, g_loss=0.774\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 67/82, d_loss_real=0.774, d_loss_fake=0.775, g_loss=0.775\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Epoch: 1, Batch: 68/82, d_loss_real=0.775, d_loss_fake=0.776, g_loss=0.776\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
            "Epoch: 1, Batch: 69/82, d_loss_real=0.776, d_loss_fake=0.777, g_loss=0.777\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 70/82, d_loss_real=0.777, d_loss_fake=0.778, g_loss=0.778\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 71/82, d_loss_real=0.777, d_loss_fake=0.779, g_loss=0.779\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 72/82, d_loss_real=0.778, d_loss_fake=0.780, g_loss=0.780\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 73/82, d_loss_real=0.779, d_loss_fake=0.780, g_loss=0.780\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 74/82, d_loss_real=0.780, d_loss_fake=0.781, g_loss=0.781\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Epoch: 1, Batch: 75/82, d_loss_real=0.781, d_loss_fake=0.782, g_loss=0.782\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Epoch: 1, Batch: 76/82, d_loss_real=0.782, d_loss_fake=0.783, g_loss=0.783\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Epoch: 1, Batch: 77/82, d_loss_real=0.782, d_loss_fake=0.784, g_loss=0.784\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 78/82, d_loss_real=0.783, d_loss_fake=0.784, g_loss=0.784\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 79/82, d_loss_real=0.784, d_loss_fake=0.785, g_loss=0.785\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 80/82, d_loss_real=0.785, d_loss_fake=0.786, g_loss=0.786\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 1, Batch: 81/82, d_loss_real=0.785, d_loss_fake=0.787, g_loss=0.787\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
            "Epoch: 1, Batch: 82/82, d_loss_real=0.786, d_loss_fake=0.787, g_loss=0.787\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Epoch: 2, Batch: 1/82, d_loss_real=0.787, d_loss_fake=0.788, g_loss=0.788\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 2/82, d_loss_real=0.788, d_loss_fake=0.789, g_loss=0.789\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 3/82, d_loss_real=0.789, d_loss_fake=0.790, g_loss=0.790\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 4/82, d_loss_real=0.789, d_loss_fake=0.790, g_loss=0.790\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 5/82, d_loss_real=0.790, d_loss_fake=0.791, g_loss=0.791\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Epoch: 2, Batch: 6/82, d_loss_real=0.791, d_loss_fake=0.792, g_loss=0.792\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Epoch: 2, Batch: 7/82, d_loss_real=0.791, d_loss_fake=0.793, g_loss=0.793\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Epoch: 2, Batch: 8/82, d_loss_real=0.792, d_loss_fake=0.793, g_loss=0.793\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 9/82, d_loss_real=0.793, d_loss_fake=0.794, g_loss=0.794\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 10/82, d_loss_real=0.794, d_loss_fake=0.795, g_loss=0.795\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 11/82, d_loss_real=0.794, d_loss_fake=0.796, g_loss=0.796\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Epoch: 2, Batch: 12/82, d_loss_real=0.795, d_loss_fake=0.796, g_loss=0.796\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
            "Epoch: 2, Batch: 13/82, d_loss_real=0.796, d_loss_fake=0.797, g_loss=0.797\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 14/82, d_loss_real=0.797, d_loss_fake=0.798, g_loss=0.798\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 15/82, d_loss_real=0.798, d_loss_fake=0.799, g_loss=0.799\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 16/82, d_loss_real=0.798, d_loss_fake=0.799, g_loss=0.799\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 17/82, d_loss_real=0.799, d_loss_fake=0.800, g_loss=0.800\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 18/82, d_loss_real=0.799, d_loss_fake=0.801, g_loss=0.801\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 19/82, d_loss_real=0.800, d_loss_fake=0.801, g_loss=0.801\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Epoch: 2, Batch: 20/82, d_loss_real=0.801, d_loss_fake=0.802, g_loss=0.802\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Epoch: 2, Batch: 21/82, d_loss_real=0.801, d_loss_fake=0.803, g_loss=0.803\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 22/82, d_loss_real=0.802, d_loss_fake=0.803, g_loss=0.803\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 23/82, d_loss_real=0.803, d_loss_fake=0.804, g_loss=0.804\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 24/82, d_loss_real=0.804, d_loss_fake=0.805, g_loss=0.805\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 25/82, d_loss_real=0.804, d_loss_fake=0.805, g_loss=0.805\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 26/82, d_loss_real=0.805, d_loss_fake=0.806, g_loss=0.806\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Epoch: 2, Batch: 27/82, d_loss_real=0.806, d_loss_fake=0.807, g_loss=0.807\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Epoch: 2, Batch: 28/82, d_loss_real=0.806, d_loss_fake=0.807, g_loss=0.807\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 29/82, d_loss_real=0.807, d_loss_fake=0.808, g_loss=0.808\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 30/82, d_loss_real=0.807, d_loss_fake=0.808, g_loss=0.808\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 31/82, d_loss_real=0.808, d_loss_fake=0.809, g_loss=0.809\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 32/82, d_loss_real=0.809, d_loss_fake=0.810, g_loss=0.810\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Epoch: 2, Batch: 33/82, d_loss_real=0.809, d_loss_fake=0.810, g_loss=0.810\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Epoch: 2, Batch: 34/82, d_loss_real=0.810, d_loss_fake=0.811, g_loss=0.811\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 35/82, d_loss_real=0.811, d_loss_fake=0.812, g_loss=0.812\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 36/82, d_loss_real=0.811, d_loss_fake=0.812, g_loss=0.812\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 37/82, d_loss_real=0.812, d_loss_fake=0.813, g_loss=0.813\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 38/82, d_loss_real=0.813, d_loss_fake=0.814, g_loss=0.814\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 39/82, d_loss_real=0.813, d_loss_fake=0.814, g_loss=0.814\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 40/82, d_loss_real=0.814, d_loss_fake=0.815, g_loss=0.815\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 41/82, d_loss_real=0.815, d_loss_fake=0.816, g_loss=0.816\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 42/82, d_loss_real=0.815, d_loss_fake=0.816, g_loss=0.816\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 43/82, d_loss_real=0.816, d_loss_fake=0.817, g_loss=0.817\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Epoch: 2, Batch: 44/82, d_loss_real=0.816, d_loss_fake=0.818, g_loss=0.818\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Epoch: 2, Batch: 45/82, d_loss_real=0.817, d_loss_fake=0.818, g_loss=0.818\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 46/82, d_loss_real=0.818, d_loss_fake=0.819, g_loss=0.819\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 47/82, d_loss_real=0.818, d_loss_fake=0.819, g_loss=0.819\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 48/82, d_loss_real=0.819, d_loss_fake=0.820, g_loss=0.820\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 49/82, d_loss_real=0.820, d_loss_fake=0.821, g_loss=0.821\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 50/82, d_loss_real=0.820, d_loss_fake=0.821, g_loss=0.821\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Epoch: 2, Batch: 51/82, d_loss_real=0.821, d_loss_fake=0.822, g_loss=0.822\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Epoch: 2, Batch: 52/82, d_loss_real=0.821, d_loss_fake=0.823, g_loss=0.823\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Epoch: 2, Batch: 53/82, d_loss_real=0.822, d_loss_fake=0.823, g_loss=0.823\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 54/82, d_loss_real=0.823, d_loss_fake=0.824, g_loss=0.824\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 55/82, d_loss_real=0.823, d_loss_fake=0.825, g_loss=0.825\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 56/82, d_loss_real=0.824, d_loss_fake=0.825, g_loss=0.825\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 57/82, d_loss_real=0.825, d_loss_fake=0.826, g_loss=0.826\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Epoch: 2, Batch: 58/82, d_loss_real=0.825, d_loss_fake=0.826, g_loss=0.826\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Epoch: 2, Batch: 59/82, d_loss_real=0.826, d_loss_fake=0.827, g_loss=0.827\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Epoch: 2, Batch: 60/82, d_loss_real=0.826, d_loss_fake=0.827, g_loss=0.827\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 61/82, d_loss_real=0.827, d_loss_fake=0.828, g_loss=0.828\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 62/82, d_loss_real=0.828, d_loss_fake=0.829, g_loss=0.829\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 63/82, d_loss_real=0.828, d_loss_fake=0.829, g_loss=0.829\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 64/82, d_loss_real=0.829, d_loss_fake=0.830, g_loss=0.830\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 65/82, d_loss_real=0.829, d_loss_fake=0.830, g_loss=0.830\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Epoch: 2, Batch: 66/82, d_loss_real=0.830, d_loss_fake=0.831, g_loss=0.831\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Epoch: 2, Batch: 67/82, d_loss_real=0.831, d_loss_fake=0.832, g_loss=0.832\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 68/82, d_loss_real=0.831, d_loss_fake=0.832, g_loss=0.832\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 69/82, d_loss_real=0.832, d_loss_fake=0.833, g_loss=0.833\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 70/82, d_loss_real=0.832, d_loss_fake=0.833, g_loss=0.833\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 71/82, d_loss_real=0.833, d_loss_fake=0.834, g_loss=0.834\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 72/82, d_loss_real=0.834, d_loss_fake=0.834, g_loss=0.834\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Epoch: 2, Batch: 73/82, d_loss_real=0.834, d_loss_fake=0.835, g_loss=0.835\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
            "Epoch: 2, Batch: 74/82, d_loss_real=0.835, d_loss_fake=0.836, g_loss=0.836\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Epoch: 2, Batch: 75/82, d_loss_real=0.835, d_loss_fake=0.836, g_loss=0.836\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 76/82, d_loss_real=0.836, d_loss_fake=0.837, g_loss=0.837\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 77/82, d_loss_real=0.836, d_loss_fake=0.837, g_loss=0.837\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 78/82, d_loss_real=0.837, d_loss_fake=0.838, g_loss=0.838\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 79/82, d_loss_real=0.837, d_loss_fake=0.838, g_loss=0.838\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 80/82, d_loss_real=0.838, d_loss_fake=0.839, g_loss=0.839\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 2, Batch: 81/82, d_loss_real=0.839, d_loss_fake=0.840, g_loss=0.840\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Epoch: 2, Batch: 82/82, d_loss_real=0.839, d_loss_fake=0.840, g_loss=0.840\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
            "Epoch: 3, Batch: 1/82, d_loss_real=0.840, d_loss_fake=0.841, g_loss=0.841\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 3, Batch: 2/82, d_loss_real=0.840, d_loss_fake=0.841, g_loss=0.841\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 3, Batch: 3/82, d_loss_real=0.841, d_loss_fake=0.842, g_loss=0.842\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 3, Batch: 4/82, d_loss_real=0.841, d_loss_fake=0.842, g_loss=0.842\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 3, Batch: 5/82, d_loss_real=0.842, d_loss_fake=0.843, g_loss=0.843\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 3, Batch: 6/82, d_loss_real=0.842, d_loss_fake=0.843, g_loss=0.843\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 3, Batch: 7/82, d_loss_real=0.843, d_loss_fake=0.844, g_loss=0.844\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Epoch: 3, Batch: 8/82, d_loss_real=0.844, d_loss_fake=0.845, g_loss=0.845\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Epoch: 3, Batch: 9/82, d_loss_real=0.844, d_loss_fake=0.845, g_loss=0.845\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
            "Epoch: 3, Batch: 10/82, d_loss_real=0.845, d_loss_fake=0.846, g_loss=0.846\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Epoch: 3, Batch: 11/82, d_loss_real=0.845, d_loss_fake=0.846, g_loss=0.846\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 3, Batch: 12/82, d_loss_real=0.846, d_loss_fake=0.847, g_loss=0.847\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 3, Batch: 13/82, d_loss_real=0.846, d_loss_fake=0.847, g_loss=0.847\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Epoch: 3, Batch: 14/82, d_loss_real=0.847, d_loss_fake=0.848, g_loss=0.848\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Function to load images from a folder\n",
        "def load_images_from_folder(folder_path, image_size=(128, 128)):\n",
        "    images = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        img_path = os.path.join(folder_path, filename)\n",
        "        img = Image.open(img_path).convert('L')  # Convert to grayscale\n",
        "        img = img.resize(image_size)  # Resize to (128, 128)\n",
        "        img_array = np.array(img)\n",
        "        images.append(img_array)\n",
        "    images = np.array(images)\n",
        "    images = np.expand_dims(images, axis=-1)  # Add channel dimension (for grayscale images)\n",
        "    return images\n",
        "\n",
        "# Load the image dataset (adjust path based on your setup)\n",
        "image_folder = r\"/content/drive/MyDrive/val\"  # Path to the folder containing your images\n",
        "data = load_images_from_folder(image_folder, image_size=(128, 128))\n",
        "\n",
        "# Normalize the data to [-1, 1]\n",
        "data = (data - 127.5) / 127.5\n",
        "\n",
        "# Constants\n",
        "NUM_EPOCHS = 10\n",
        "LATENT_DIM = 100\n",
        "INPUT_SHAPE = (128, 128, 1)\n",
        "LEARNING_RATE = 0.0002\n",
        "MOMENTUM = 0.5\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Define the Generator (same as before)\n",
        "def define_generator(latent_dim):\n",
        "    model = Model()\n",
        "    # Latent input\n",
        "    in_lat = layers.Input(shape=(latent_dim,))\n",
        "    # Foundation for 8x8 image\n",
        "    n_nodes = 128 * 8 * 8\n",
        "    gen = layers.Dense(n_nodes)(in_lat)\n",
        "    gen = layers.LeakyReLU(alpha=0.2)(gen)\n",
        "    gen = layers.Reshape((8, 8, 128))(gen)\n",
        "\n",
        "    # Upsample to 16x16\n",
        "    gen = layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n",
        "    gen = layers.LeakyReLU(alpha=0.2)(gen)\n",
        "\n",
        "    # Upsample to 32x32\n",
        "    gen = layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n",
        "    gen = layers.LeakyReLU(alpha=0.2)(gen)\n",
        "\n",
        "    # Upsample to 64x64\n",
        "    gen = layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n",
        "    gen = layers.LeakyReLU(alpha=0.2)(gen)\n",
        "\n",
        "    # Upsample to 128x128\n",
        "    gen = layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n",
        "    gen = layers.LeakyReLU(alpha=0.2)(gen)\n",
        "\n",
        "    # Output layer - single channel (grayscale) with tanh activation\n",
        "    out_layer = layers.Conv2D(1, (7,7), activation='tanh', padding='same')(gen)\n",
        "\n",
        "    # Define model\n",
        "    model = Model(in_lat, out_layer)\n",
        "    return model\n",
        "\n",
        "# Define the Discriminator (same as before)\n",
        "def define_discriminator(in_shape=(128, 128, 1)):\n",
        "    model = Model()\n",
        "    # Image input\n",
        "    in_image = layers.Input(shape=in_shape)\n",
        "\n",
        "    # Downsample to 64x64\n",
        "    fe = layers.Conv2D(64, (4,4), strides=(2,2), padding='same')(in_image)\n",
        "    fe = layers.LeakyReLU(alpha=0.2)(fe)\n",
        "\n",
        "    # Downsample to 32x32\n",
        "    fe = layers.Conv2D(128, (4,4), strides=(2,2), padding='same')(fe)\n",
        "    fe = layers.LeakyReLU(alpha=0.2)(fe)\n",
        "\n",
        "    # Downsample to 16x16\n",
        "    fe = layers.Conv2D(128, (4,4), strides=(2,2), padding='same')(fe)\n",
        "    fe = layers.LeakyReLU(alpha=0.2)(fe)\n",
        "\n",
        "    # Downsample to 8x8\n",
        "    fe = layers.Conv2D(128, (4,4), strides=(2,2), padding='same')(fe)\n",
        "    fe = layers.LeakyReLU(alpha=0.2)(fe)\n",
        "\n",
        "    # Flatten feature maps\n",
        "    fe = layers.Flatten()(fe)\n",
        "\n",
        "    # Output\n",
        "    out_layer = layers.Dense(1, activation='sigmoid')(fe)\n",
        "\n",
        "    # Define model\n",
        "    model = Model(in_image, out_layer)\n",
        "\n",
        "    # Compile model\n",
        "    opt = Adam(learning_rate=0.00001, beta_1=MOMENTUM)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Define the GAN (same as before)\n",
        "def define_gan(generator, discriminator):\n",
        "    # Make discriminator not trainable when training the GAN\n",
        "    discriminator.trainable = False\n",
        "\n",
        "    # GAN input (noise)\n",
        "    gan_input = layers.Input(shape=(LATENT_DIM,))\n",
        "\n",
        "    # Output of generator\n",
        "    gen_output = generator(gan_input)\n",
        "\n",
        "    # Output of discriminator (fake image classification)\n",
        "    gan_output = discriminator(gen_output)\n",
        "\n",
        "    # Define GAN model\n",
        "    gan_model = Model(gan_input, gan_output)\n",
        "\n",
        "    # Compile GAN\n",
        "    opt = Adam(learning_rate=LEARNING_RATE, beta_1=MOMENTUM)\n",
        "    gan_model.compile(loss='binary_crossentropy', optimizer=opt)\n",
        "\n",
        "    return gan_model\n",
        "\n",
        "# Function to generate latent points for the generator (same as before)\n",
        "def generate_latent_points(latent_dim, n_samples):\n",
        "    x_input = np.random.randn(latent_dim * n_samples)\n",
        "    x_input = x_input.reshape(n_samples, latent_dim)\n",
        "    return x_input\n",
        "\n",
        "# Generate fake samples (using the generator) (same as before)\n",
        "def generate_fake_samples(generator, latent_dim, n_samples):\n",
        "    latent_points = generate_latent_points(latent_dim, n_samples)\n",
        "    images = generator.predict(latent_points)\n",
        "    y = np.zeros((n_samples, 1))\n",
        "    return images, y\n",
        "\n",
        "# Train the GAN (same as before)\n",
        "def train_gan(generator, discriminator, gan_model, dataset, latent_dim, n_epochs=100, n_batch=64):\n",
        "    batch_per_epoch = int(dataset.shape[0] / n_batch)\n",
        "    half_batch = int(n_batch / 2)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        for batch in range(batch_per_epoch):\n",
        "            # Get real samples\n",
        "            ix = np.random.randint(0, dataset.shape[0], half_batch)\n",
        "            X_real, y_real = dataset[ix], np.ones((half_batch, 1))\n",
        "\n",
        "            # Get fake samples\n",
        "            X_fake, y_fake = generate_fake_samples(generator, latent_dim, half_batch)\n",
        "\n",
        "            # Update discriminator with real and fake samples\n",
        "            # Update discriminator with real and fake samples\n",
        "            d_loss_real, _ = discriminator.train_on_batch(X_real, y_real)\n",
        "            d_loss_fake, _ = discriminator.train_on_batch(X_fake, y_fake)\n",
        "\n",
        "            # Ensure you take only the first value (the loss) if train_on_batch returns a list\n",
        "            d_loss_real = d_loss_real[0] if isinstance(d_loss_real, list) else d_loss_real\n",
        "            d_loss_fake = d_loss_fake[0] if isinstance(d_loss_fake, list) else d_loss_fake\n",
        "\n",
        "            # Prepare points in latent space for the GAN\n",
        "            X_gan = generate_latent_points(latent_dim, n_batch)\n",
        "            y_gan = np.ones((n_batch, 1))\n",
        "\n",
        "            #  Update the generator via the discriminator's error\n",
        "            g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
        "            g_loss = g_loss[0] if isinstance(g_loss, list) else g_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            # Summarize loss on this batch\n",
        "            print(f'Epoch: {epoch+1}, Batch: {batch+1}/{batch_per_epoch}, d_loss_real={d_loss_real:.3f}, d_loss_fake={d_loss_fake:.3f}, g_loss={g_loss:.3f}')\n",
        "\n",
        "        # Save generated images periodically\n",
        "        if (epoch+1) % 10 == 0:\n",
        "            save_plot(generator, latent_dim, epoch)\n",
        "\n",
        "# Function to save generated images (same as before)\n",
        "def save_plot(generator, latent_dim, epoch, n=10):\n",
        "    latent_points = generate_latent_points(latent_dim, n*n)\n",
        "    images = generator.predict(latent_points)\n",
        "\n",
        "    # Rescale pixel values to [0,1]\n",
        "    images = (images + 1) / 2.0\n",
        "\n",
        "    # Plot images\n",
        "    for i in range(n * n):\n",
        "        plt.subplot(n, n, 1 + i)\n",
        "        plt.axis('off')\n",
        "        plt.imshow(images[i, :, :, 0], cmap='gray')\n",
        "\n",
        "    # Save plot to file\n",
        "    filename = f'generated_plot_e{epoch+1}.png'\n",
        "    plt.savefig(filename)\n",
        "    plt.close()\n",
        "\n",
        "# Create the models\n",
        "discriminator = define_discriminator()\n",
        "generator = define_generator(LATENT_DIM)\n",
        "gan_model = define_gan(generator, discriminator)\n",
        "\n",
        "# Train the GAN\n",
        "def train_gan(generator, discriminator, gan_model, dataset, latent_dim, n_epochs=100, n_batch=64):\n",
        "    batch_per_epoch = int(dataset.shape[0] / n_batch)\n",
        "    half_batch = int(n_batch / 2)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        for batch in range(batch_per_epoch):\n",
        "            # Get real samples\n",
        "            ix = np.random.randint(0, dataset.shape[0], half_batch)\n",
        "            X_real, y_real = dataset[ix], np.ones((half_batch, 1))\n",
        "\n",
        "            # Get fake samples\n",
        "            X_fake, y_fake = generate_fake_samples(generator, latent_dim, half_batch)\n",
        "\n",
        "            # Update discriminator with real and fake samples\n",
        "            d_loss_real, _ = discriminator.train_on_batch(X_real, y_real)\n",
        "            d_loss_fake, _ = discriminator.train_on_batch(X_fake, y_fake)\n",
        "\n",
        "            # Prepare points in latent space for the GAN\n",
        "            X_gan = generate_latent_points(latent_dim, n_batch)\n",
        "            y_gan = np.ones((n_batch, 1))\n",
        "\n",
        "            # Update the generator via the discriminator's error\n",
        "            g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
        "\n",
        "            # Summarize loss on this batch\n",
        "            print(f'Epoch: {epoch+1}, Batch: {batch+1}/{batch_per_epoch}, '\n",
        "              f'd_loss_real={d_loss_real[0] if isinstance(d_loss_real, list) else d_loss_real:.3f}, '\n",
        "              f'd_loss_fake={d_loss_fake[0] if isinstance(d_loss_fake, list) else d_loss_fake:.3f}, '\n",
        "              f'g_loss={g_loss[0] if isinstance(g_loss, list) else g_loss:.3f}')\n",
        "\n",
        "        # Save generated images periodically\n",
        "        if (epoch+1) % 10 == 0:\n",
        "            save_plot(generator, latent_dim, epoch)\n",
        "\n",
        "# Function to save generated images\n",
        "def save_plot(generator, latent_dim, epoch, n=10):\n",
        "    latent_points = generate_latent_points(latent_dim, n*n)\n",
        "    images = generator.predict(latent_points)\n",
        "\n",
        "    # Rescale pixel values to [0,1]\n",
        "    images = (images + 1) / 2.0\n",
        "\n",
        "    # Plot images\n",
        "    for i in range(n * n):\n",
        "        plt.subplot(n, n, 1 + i)\n",
        "        plt.axis('off')\n",
        "        plt.imshow(images[i, :, :, 0], cmap='gray')\n",
        "\n",
        "    # Save plot to file\n",
        "    filename = f'generated_plot_e{epoch+1}.png'\n",
        "    plt.savefig(filename)\n",
        "    plt.close()\n",
        "\n",
        "# Create the models\n",
        "discriminator = define_discriminator()\n",
        "generator = define_generator(LATENT_DIM)\n",
        "gan_model = define_gan(generator, discriminator)\n",
        "\n",
        "# Train the GAN\n",
        "train_gan(generator, discriminator, gan_model, data, LATENT_DIM, NUM_EPOCHS, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "4qyig-Me7_P0",
        "outputId": "6a2c5aa1-f6d5-4e33-fc8c-debb7cdb396e"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'latent_points' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b8431206552f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Rescale pixel values to [0,1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'latent_points' is not defined"
          ]
        }
      ],
      "source": [
        "images = generator.predict(latent_points)\n",
        "\n",
        "    # Rescale pixel values to [0,1]\n",
        "images = (images + 1) / 2.0\n",
        "\n",
        "    # Plot images\n",
        "for i in range(n * n):\n",
        "        plt.subplot(n, n, 1 + i)\n",
        "        plt.axis('off')\n",
        "        plt.imshow(images[i, :, :, 0], cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiKAbHOc00kK"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZO180hk5I_Sk",
        "outputId": "70d2d221-73f6-4d6c-f951-e498bfa65469"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:TPU system local has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Function to load images from a folder\n",
        "def load_images_from_folder(folder_path, image_size=(128, 128)):\n",
        "    images = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        img_path = os.path.join(folder_path, filename)\n",
        "        img = Image.open(img_path).convert('L')  # Convert to grayscale\n",
        "        img = img.resize(image_size)  # Resize to (128, 128)\n",
        "        img_array = np.array(img)\n",
        "        images.append(img_array)\n",
        "    images = np.array(images)\n",
        "    images = np.expand_dims(images, axis=-1)  # Add channel dimension (for grayscale images)\n",
        "    return images\n",
        "\n",
        "# Load the image dataset (adjust path based on your setup)\n",
        "image_folder = r\"/content/drive/MyDrive/val\"  # Path to the folder containing your images\n",
        "data = load_images_from_folder(image_folder, image_size=(128, 128))\n",
        "\n",
        "# Normalize the data to [-1, 1]\n",
        "data = (data - 127.5) / 127.5\n",
        "\n",
        "# Constants\n",
        "NUM_EPOCHS = 100\n",
        "LATENT_DIM = 100\n",
        "INPUT_SHAPE = (128, 128, 1)\n",
        "LEARNING_RATE = 0.0002\n",
        "MOMENTUM = 0.5\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Detect TPU and initialize\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # Get TPU system\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)  # Strategy to distribute the model on TPU\n",
        "    print(\"TPU initialized.\")\n",
        "except ValueError:\n",
        "    strategy = tf.distribute.get_strategy()  # Default strategy for CPU and single GPU\n",
        "    print(\"Using default strategy.\")\n",
        "\n",
        "# Define the Generator\n",
        "def define_generator(latent_dim):\n",
        "    model = Model()\n",
        "    # Latent input\n",
        "    in_lat = layers.Input(shape=(latent_dim,))\n",
        "    # Foundation for 8x8 image\n",
        "    n_nodes = 128 * 8 * 8\n",
        "    gen = layers.Dense(n_nodes)(in_lat)\n",
        "    gen = layers.LeakyReLU(alpha=0.2)(gen)\n",
        "    gen = layers.Reshape((8, 8, 128))(gen)\n",
        "\n",
        "    # Upsample to 128x128\n",
        "    for _ in range(4):\n",
        "        gen = layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same')(gen)\n",
        "        gen = layers.LeakyReLU(alpha=0.2)(gen)\n",
        "\n",
        "    # Output layer - single channel (grayscale) with tanh activation\n",
        "    out_layer = layers.Conv2D(1, (7, 7), activation='tanh', padding='same')(gen)\n",
        "\n",
        "    # Define model\n",
        "    model = Model(in_lat, out_layer)\n",
        "    return model\n",
        "\n",
        "# Define the Discriminator\n",
        "def define_discriminator(in_shape=(128, 128, 1)):\n",
        "    model = Model()\n",
        "    # Image input\n",
        "    in_image = layers.Input(shape=in_shape)\n",
        "\n",
        "    # Downsample to 8x8\n",
        "    for _ in range(4):\n",
        "        fe = layers.Conv2D(128, (4, 4), strides=(2, 2), padding='same')(in_image if _ == 0 else fe)\n",
        "        fe = layers.LeakyReLU(alpha=0.2)(fe)\n",
        "\n",
        "    # Flatten feature maps\n",
        "    fe = layers.Flatten()(fe)\n",
        "\n",
        "    # Output\n",
        "    out_layer = layers.Dense(1, activation='sigmoid')(fe)\n",
        "\n",
        "    # Define model\n",
        "    model = Model(in_image, out_layer)\n",
        "\n",
        "    # Compile model\n",
        "    opt = Adam(learning_rate=0.0001, beta_1=MOMENTUM)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Define the GAN\n",
        "def define_gan(generator, discriminator):\n",
        "    # Make discriminator not trainable when training the GAN\n",
        "    discriminator.trainable = False\n",
        "\n",
        "    # GAN input (noise)\n",
        "    gan_input = layers.Input(shape=(LATENT_DIM,))\n",
        "\n",
        "    # Output of generator\n",
        "    gen_output = generator(gan_input)\n",
        "\n",
        "    # Output of discriminator (fake image classification)\n",
        "    gan_output = discriminator(gen_output)\n",
        "\n",
        "    # Define GAN model\n",
        "    gan_model = Model(gan_input, gan_output)\n",
        "\n",
        "    # Compile GAN\n",
        "    opt = Adam(learning_rate=LEARNING_RATE, beta_1=MOMENTUM)\n",
        "    gan_model.compile(loss='binary_crossentropy', optimizer=opt)\n",
        "\n",
        "    return gan_model\n",
        "\n",
        "# Function to generate latent points for the generator\n",
        "def generate_latent_points(latent_dim, n_samples):\n",
        "    x_input = np.random.randn(latent_dim * n_samples)\n",
        "    x_input = x_input.reshape(n_samples, latent_dim)\n",
        "    return x_input\n",
        "\n",
        "# Generate fake samples\n",
        "def generate_fake_samples(generator, latent_dim, n_samples):\n",
        "    latent_points = generate_latent_points(latent_dim, n_samples)\n",
        "    images = generator.predict(latent_points)\n",
        "    y = np.zeros((n_samples, 1))\n",
        "    return images, y\n",
        "\n",
        "# Function to save generated images\n",
        "def save_plot(generator, latent_dim, epoch, n=10):\n",
        "    latent_points = generate_latent_points(latent_dim, n*n)\n",
        "    images = generator.predict(latent_points)\n",
        "\n",
        "    # Rescale pixel values to [0,1]\n",
        "    images = (images + 1) / 2.0\n",
        "\n",
        "    # Plot images\n",
        "    for i in range(n * n):\n",
        "        plt.subplot(n, n, 1 + i)\n",
        "        plt.axis('off')\n",
        "        plt.imshow(images[i, :, :, 0], cmap='gray')\n",
        "\n",
        "    # Save plot to file\n",
        "    filename = f'generated_plot_e{epoch+1}.png'\n",
        "    plt.savefig(filename)\n",
        "    plt.close()\n",
        "\n",
        "# Train the GAN\n",
        "# Define the GAN training function with checkpoints\n",
        "def train_gan(generator, discriminator, gan_model, dataset, latent_dim, n_epochs=100, n_batch=64, checkpoint_dir='checkpoints/'):\n",
        "    batch_per_epoch = int(dataset.shape[0] / n_batch)\n",
        "    half_batch = int(n_batch / 2)\n",
        "\n",
        "    # Create the checkpoint directory if it doesn't exist\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        os.makedirs(checkpoint_dir)\n",
        "\n",
        "    # Define a checkpoint callback for the generator\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "        filepath= '/content/drive/MyDrive/gan_generator_epoch_{epoch:03d}.h5',\n",
        "        save_weights_only=True,\n",
        "        save_freq='epoch',\n",
        "        period=10  # Save every 10 epochs\n",
        "    )\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        for batch in range(batch_per_epoch):\n",
        "            # Get real samples\n",
        "            ix = np.random.randint(0, dataset.shape[0], half_batch)\n",
        "            X_real, y_real = dataset[ix], np.ones((half_batch, 1))\n",
        "\n",
        "            # Get fake samples\n",
        "            X_fake, y_fake = generate_fake_samples(generator, latent_dim, half_batch)\n",
        "\n",
        "            # Update discriminator with real and fake samples\n",
        "            d_loss_real = discriminator.train_on_batch(X_real, y_real)\n",
        "            d_loss_fake = discriminator.train_on_batch(X_fake, y_fake)\n",
        "\n",
        "            # Prepare points in latent space for the GAN\n",
        "            X_gan = generate_latent_points(latent_dim, n_batch)\n",
        "            y_gan = np.ones((n_batch, 1))\n",
        "\n",
        "            # Update the generator via the discriminator's error\n",
        "            g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
        "\n",
        "            # Summarize loss on this batch\n",
        "            print(f'Epoch: {epoch+1}, Batch: {batch+1}/{batch_per_epoch}, '\n",
        "                f'd_loss_real={d_loss_real[0] if isinstance(d_loss_real, list) else d_loss_real:.3f}, '\n",
        "                f'd_loss_fake={d_loss_fake[0] if isinstance(d_loss_fake, list) else d_loss_fake:.3f}, '\n",
        "                f'g_loss={g_loss[0] if isinstance(g_loss, list) else g_loss:.3f}')\n",
        "\n",
        "        # Save generated images periodically\n",
        "        if (epoch+1) % 10 == 0:\n",
        "            save_plot(generator, latent_dim, epoch)\n",
        "\n",
        "        # Save the model weights periodically using the callback\n",
        "        checkpoint_callback.on_epoch_end(epoch)\n",
        "\n",
        "\n",
        "# Create the models within the TPU strategy scope\n",
        "with strategy.scope():\n",
        "    discriminator = define_discriminator()\n",
        "    generator = define_generator(LATENT_DIM)\n",
        "    gan_model = define_gan(generator, discriminator)\n",
        "\n",
        "# Train the GAN\n",
        "train_gan(generator, discriminator, gan_model, data, LATENT_DIM, NUM_EPOCHS, BATCH_SIZE)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}